---
title: "Swiftkey Word Prediction"
author: "Sheng Feng"
date: "4/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

library(ggplot2)
library(dplyr)
library(tidytext)
library(printr)
```

## Introduction

This is the capstone project for the Data Science Specialization. In this work we foucs on Natural Languange Processing and word prediction. 


## Loading data
Here we load the data from the [original dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip), make a sample that consists 20% of the records, and save the samples.

```{r sampling}
set.seed(5832)

processFile <- function(filepath, sampleProp) {
  df = data.frame(readLines(filepath))
  return(sample_frac(df, sampleProp))
}


dir.create(file.path(".", "sample"), showWarnings = FALSE)
if(!file.exists("sample/twitter_sample.txt")){
  twsample <- processFile("final/en_US/en_US.twitter.txt", 0.2)
  write.table(twsample, "sample/twitter_sample.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
}
if(!file.exists("sample/blogs_sample.txt")){
  blogsample <- processFile("final/en_US/en_US.blogs.txt",  0.2)
  write.table(blogsample, "sample/blogs_sample.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
}
if(!file.exists("sample/news_sample.txt")){
  newsample <- processFile("final/en_US/en_US.news.txt", 0.2)
  write.table(newsample, "sample/news_sample.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
}
```

Here we devides the sample into three subset by 60% / 20% / 20%, and use them as training, testing and validation sets respectively.
```{r dividing.samples}
if(!file.exists("data/training.rds")){
  tw <- readLines("sample/twitter_sample.txt")
  bl <- readLines("sample/blogs_sample.txt")
  nw <- readLines("sample/news_sample.txt")
  text = c(tw, bl, nw)
  Encoding(text) <- "UTF-8"
  docs <- data_frame(text)
  set.seed(4869)
  intrain <- sample(nrow(docs), 0.6 * nrow(docs))
  training <- docs[intrain,]
  dir.create(file.path(".", "data"), showWarnings = FALSE)
  saveRDS(training, "data/training.rds")
  testing <- docs[-intrain, ]
  invalid <- sample(nrow(testing), 0.5 * nrow(testing))
  validating <- testing[invalid,]
  testing <- testing[-invalid,]
  saveRDS(validating, "data/validating.rds")
  saveRDS(testing, "data/testing.rds")
}
```

## Unigram Exploration
We tokenize the text samples into words, remove the bad words and stop words, and do some exploratory analysis.
```{r unigram.exploration}
training <- readRDS("data/training.rds")
tidy_doc <- training %>% unnest_tokens(word, text) %>%
        filter(!grepl("[+-]?([0-9]*[.])?[0-9]+", word))
```
```{r remove.bad.words}
bad.words <- data_frame(word = as.character(read.table("google_twunter_lol", skip = 1, sep = ":", nrow = 451)$V1))
tidy_doc <- tidy_doc %>% anti_join(bad.words)
```
```{r remove.stop.words}
data(stop_words)
tidystop <- tidy_doc %>% anti_join(stop_words)
```

```{r word.count}
tidystop %>%  count(word, sort = TRUE) %>%
   .[1:30,] %>%
   mutate(word = reorder(word, n)) %>%
   ggplot(aes(word, n)) +
   geom_col() +
   xlab(NULL) +
   coord_flip()
```

```{r word.cloud}
library(wordcloud)
tidystop %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 120))
```
```{r comparison.cloud}
library(reshape2)
tidystop %>%
 inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```

## Generating n-grams
We tokenize the text into bigrams, trigrams and quadgrams, and remove the tokens that contain bad words. We do not remove the stop words since they would be valid predictions.

```{r generate.bigrams}
mybigrams <- training %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) 
```
```{r filter.bigrams}
library(tidyr)
bigrams_separated <- mybigrams %>%
            filter(!grepl("[+-]?([0-9]*[.])?[0-9]+", bigram)) %>% 
            separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% bad.words$word) %>%
  filter(!word2 %in% bad.words$word)
```

```{r top.bigrams}
bigramTable <- bigrams_filtered %>% 
                count(word1, word2, sort = TRUE) %>% 
                ungroup() %>%
                rename(first = word1, last = word2, freq = n)
```


```{r trigrams}
mytrigrams <- training %>% unnest_tokens(trigram, text, token = "ngrams", n = 3) 
```
```{r filter.trigrams}
trigrams_separated <- mytrigrams %>%
            filter(!grepl("[+-]?([0-9]*[.])?[0-9]+", trigram)) %>% 
            separate(trigram, c("word1", "word2","word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% bad.words$word) %>%
  filter(!word2 %in% bad.words$word) %>%
  filter(!word3 %in% bad.words$word)
```

```{r top.trigrams}
trigramTable <- trigrams_filtered %>% count(word1, word2, word3, sort = TRUE) %>% rename(last = word3, freq = n) %>% mutate(first = paste(word1, word2)) %>% ungroup() %>% select(first, last, freq)
```


```{r quadgrams}
quadgramTable <- training %>% unnest_tokens(quadgram, text, token = "ngrams", n = 4)  %>%
        filter(!grepl("[+-]?([0-9]*[.])?[0-9]+", quadgram)) %>% 
        separate(quadgram, c("word1", "word2","word3","word4"), sep = " ") %>%
        filter(!word1 %in% bad.words$word) %>%
        filter(!word2 %in% bad.words$word) %>%
        filter(!word3 %in% bad.words$word) %>%
        filter(!word4 %in% bad.words$word) %>%
        count(word1, word2, word3, word4, sort = TRUE) %>% 
        rename(last = word4, freq = n) %>% 
        mutate(first = paste(word1, word2, word3)) %>% 
        ungroup() %>% 
        select(first, last, freq)
```

```{r view.bigram.trigram}
head(bigramTable)
head(trigramTable)
head(quadgramTable)
```


## Good-Turing Discount

```{r GT.discount}
calcDiscount <- function(ngramTable){

  freqTable <- ngramTable %>% group_by(freq) %>% summarise(n = n()) %>% ungroup() %>% arrange(freq)
  freqTable$discount <- 1
  for(i in 1:5){
    currR = i
    nextR = i + 1
    currN = freqTable$n[i]
    nextN = freqTable$n[i + 1]
    freqTable$discount[i] <- (nextR / currR) * (nextN / currN)
  }
  freqTable <- freqTable %>% select(freq, discount)
  ngramTable %>% left_join(freqTable)
}

bigramTable <- calcDiscount(bigramTable)
trigramTable <- calcDiscount(trigramTable)
quadgramTable <- calcDiscount(quadgramTable)
```

```{r save.data.for.app}
dir.create(file.path(".", "ngrams"), showWarnings = FALSE)
saveRDS(bigramTable, "ngrams/bigrams.rds")
saveRDS(trigramTable, "ngrams/trigrams.rds")
saveRDS(quadgramTable, "ngrams/quadgrams.rds")
```


## Calculate Conditional Probability with Katz Backoff
```{r backoff.prob}
calcProb <- function(f, l){
  three = tail(unlist(strsplit(f," ")),3)
  leftover <- 1
  quadtab <- quadgramTable %>% filter(first == paste(three, collapse = " "))
  r <-  quadtab %>% filter(last == l)
  if (nrow(r) > 0) {
    print("quadgram hit!")
    d = r$discount[1]
    n = r$freq[1]
    return(d * n / sum(quadtab$freq))
  }

  if (nrow(quadtab) > 1)
    leftover <-  1 - sum(quadtab$discount * quadtab$freq) / sum(quadtab$freq)
  tritab <- trigramTable %>% filter(first == paste(tail(three,2), collapse = " "))
  tritab <- tritab %>% anti_join(quadtab, by = "last")
  r <-  tritab %>% filter(last == l)
  if (nrow(r) > 0) {
    print("trigram hit!")
    d = r$discount[1]
    n = r$freq[1]
    return(leftover * d * n / sum(tritab$freq))
  }
  
  if (nrow(tritab) > 1)
    leftover <-  leftover * ( 1 - sum(tritab$discount * tritab$freq) / sum(tritab$freq))
  
  bitab <- bigramTable %>% filter(first == tail(three, 1))
  bitab <- bitab %>% anti_join(quadtab, by = "last")
  bitab <- bitab %>% anti_join(tritab, by = "last")
  r <-  bitab %>% filter(last == l)
  if (nrow(r) > 0) {
    print("bigram hit!")
    d = r$discount[1]
    n = r$freq[1]
    return(leftover * d * n / sum(bitab$freq))
  }

  leftover <- leftover * ( 1 - sum(bitab$discount * bitab$freq) / sum(bitab$freq))
  print("bigram miss!")
  return(leftover)
}

calcProb("i am", "a")
```

```{r backoff.prediction}
backoffPred <- function(f){
  three = tail(unlist(strsplit(f," ")),3)
  leftover <- 1
  
  pred = data_frame()
  quadtab <- quadgramTable %>% filter(first == paste(three, collapse = " "))
  
  if (nrow(quadtab) > 0) {
    # print("quadgram hit!")
    pred <- quadtab %>% mutate(prob = freq * discount / sum(quadtab$freq)) %>% select(last, prob)
    leftover <-  1 - sum(pred$prob)
    if (leftover < 0.1)
      return(pred %>% arrange(desc(prob)) %>% head(10))
  }
 
  
  tritab <- trigramTable %>% filter(first == paste(tail(three,2), collapse = " ")) %>% anti_join(quadtab, by = "last")
  
  if (nrow(tritab) > 0) {
    # print("trigram hit!")
    pred <- rbind(pred, tritab %>% mutate(prob = leftover * freq * discount / sum(tritab$freq)) %>% select(last, prob))
    leftover <-  1 - sum(pred$prob)
    if (leftover < 0.1)
      return(pred %>% arrange(desc(prob)) %>% head(10))
  }
  
  bitab <- bigramTable %>% filter(first == tail(three, 1)) %>% anti_join(quadtab, by = "last") %>% anti_join(tritab, by = "last")

  if (nrow(bitab) > 0) {
    # print("bigram hit!")
    pred <- rbind(pred, bitab %>% ungroup() %>% mutate(prob = leftover * freq * discount / sum(bitab$freq)) %>% select(last, prob))
  }
  
  return(pred %>% arrange(desc(prob)) %>% head(10))
}

backoffPred("this is a")
```

## Model evaluation
We use the test dataset to evaluate our n-gram model. 
```{r Measure.performance}

```

## Referece 
[tidytext](http://tidytextmining.com/)  
[ngram](https://en.wikipedia.org/wiki/N-gram)  
[katz's back off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model)  
[bad words](https://gist.github.com/jamiew/1112488)


