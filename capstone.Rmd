---
title: "Swiftkey Word Prediction"
author: "Sheng Feng"
date: "4/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

library(printr)
library(ggplot2)
library(tm)
library(dplyr)
library(data.table)
library(tidytext)
```

## Introduction

This is the capstone project for the Data Science Specialization. In this work we foucs on Natural Languange Processing and word prediction. 

## Referece 
[tidytext](http://tidytextmining.com/)  
[ngram](https://en.wikipedia.org/wiki/N-gram)  
[bad words](https://gist.github.com/jamiew/1112488)

## Loading data
```{r sampling}
processFile <- function(filepath, prob, max) {
  con <- file(filepath, "r") 
  datalist = rep("", max)
  count = 0
  while (length(oneLine <- readLines(con, n = 1, warn = FALSE)) > 0 & count < max) {
    if (runif(1) < prob){
      count = count + 1
      datalist[count]  <- oneLine
      count = count
    }
  } 

  close(con) ## It's important to close the connection when you are done

  return(data.frame(lines = datalist[1:count]))
}

set.seed(5832)
dir.create(file.path(".", "sample"), showWarnings = FALSE)
if(!file.exists("sample/twitter_sample.txt")){
  twsample <- processFile("final/en_US/en_US.twitter.txt", prob = 0.2, 200000)
  write.table(twsample, "sample/twitter_sample.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
}
if(!file.exists("sample/blogs_sample.txt")){
  blogsample <- processFile("final/en_US/en_US.blogs.txt", prob = 0.2, 200000)
  write.table(blogsample, "sample/blogs_sample.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
}
if(!file.exists("sample/news_sample.txt")){
  newsample <- processFile("final/en_US/en_US.news.txt", prob = 0.2, 200000)
  write.table(newsample, "sample/news_sample.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
}
```


```{r load.samples}
tw <- readLines("sample/twitter_sample.txt")
bl <- readLines("sample/blogs_sample.txt")
nw <- readLines("sample/news_sample.txt")
docs <- data_frame(text = c(tw,bl,nw))
tidy_doc <- docs %>% unnest_tokens(word, text) %>%
        filter(!grepl("[0-9]+", word))
```
```{r remove.bad.words}
bad.words <- data_frame(word = as.character(read.table("google_twunter_lol", skip = 1, sep = ":", nrow = 451)$V1))
tidy_doc <- tidy_doc %>% anti_join(bad.words)
```
```{r remove.stop.words}
data(stop_words)
tidystop <- tidy_doc %>% anti_join(stop_words)
```

```{r word.count}
tidystop %>%  count(word, sort = TRUE) %>%
   .[1:30,] %>%
   mutate(word = reorder(word, n)) %>%
   ggplot(aes(word, n)) +
   geom_col() +
   xlab(NULL) +
   coord_flip()
```

```{r word.cloud}
library(wordcloud)
tidystop %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
```{r comparison.cloud}
library(reshape2)
tidystop %>%
 inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```





